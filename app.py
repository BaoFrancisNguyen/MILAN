import logging
from flask import Flask, render_template, request, redirect, url_for, flash, session, jsonify
from flask_session import Session
from werkzeug.utils import secure_filename
import os
import uuid
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Importer les modules personnalis√©s
from modules.audio_processor_module import LocalAudioProcessor
from modules.data_processor_module import DataProcessor
from modules.pdf_processor_module import PDFProcessor  
from modules.visualization_module import create_visualization
from modules.data_transformer_module import DataTransformer
from modules.history_manager_module import AnalysisHistory, PDFAnalysisHistory
from modules.signal_processor import process_audio_signal

def convert_numpy_types(obj):
    """Convertit les types NumPy en types Python standards pour la s√©rialisation JSON"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list) or isinstance(obj, tuple):
        return [convert_numpy_types(i) for i in obj]
    else:
        return obj

# Configuration de l'application
app = Flask(__name__)
app.secret_key = os.environ.get('SECRET_KEY', 'orbital')  
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['ALLOWED_EXTENSIONS'] = {'csv', 'pdf'}
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max
app.config['HISTORY_FOLDER'] = 'analysis_history'

# Configuration de la session
app.config['SESSION_TYPE'] = 'filesystem'
app.config['SESSION_PERMANENT'] = True
app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=30)
Session(app)

# Cr√©er les dossiers n√©cessaires
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs(app.config['HISTORY_FOLDER'], exist_ok=True)
os.makedirs(os.path.join(app.config['HISTORY_FOLDER'], 'pdf'), exist_ok=True)

# Configuration du logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Initialisation des gestionnaires d'historique
csv_history = AnalysisHistory(app.config['HISTORY_FOLDER'])
pdf_history = PDFAnalysisHistory(os.path.join(app.config['HISTORY_FOLDER'], 'pdf'))

# Initialisation du transformateur de donn√©es
data_transformer = DataTransformer(
    os.environ.get('AI_MODEL', 'mistral:latest'),
    int(os.environ.get('CONTEXT_SIZE', '4096'))
)

def allowed_file(filename):
    """V√©rifie si le type de fichier est autoris√©"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

@app.before_request
def make_session_permanent():
    """Rend la session permanente"""
    session.permanent = True

@app.route('/apply_transformation', methods=['POST'])
def apply_transformation():
    """
    Route pour appliquer une transformation sur le DataFrame
    """
    # V√©rifier l'authentification et la session
    if 'current_file' not in session or session['current_file']['type'] != 'csv':
        return jsonify({
            'success': False, 
            'error': 'Aucun fichier CSV charg√©',
            'redirect': url_for('data_processing')
        }), 400

    try:
        # R√©cup√©rer les donn√©es de transformation
        transformation_data = request.json
        
        # V√©rifier la pr√©sence des donn√©es requises
        if not transformation_data or 'type' not in transformation_data:
            return jsonify({
                'success': False, 
                'error': 'Donn√©es de transformation invalides'
            }), 400

        # Charger le DataFrame actuel
        file_path = session['current_file']['path']
        df = pd.read_csv(file_path)

        # Initialiser le processeur de donn√©es
        processor = DataProcessor()

        # Pr√©parer les param√®tres de transformation
        transform_params = {
            transformation_data['type']: transformation_data.get('details', {})
        }

        # Appliquer la transformation
        try:
            transformed_df, metadata = processor.process_dataframe(
                df, 
                transformations=transform_params
            )

            # G√©n√©rer un nom de fichier unique pour la version transform√©e
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            new_filename = f"transformed_{timestamp}_{os.path.basename(file_path)}"
            new_file_path = os.path.join(app.config['UPLOAD_FOLDER'], new_filename)

            # Sauvegarder le DataFrame transform√©
            transformed_df.to_csv(new_file_path, index=False)

            # Mettre √† jour la session avec le nouveau fichier
            session['current_file'] = {
                'name': new_filename,
                'path': new_file_path,
                'type': 'csv'
            }
            session.modified = True

            # G√©rer l'historique des transformations
            if 'transformations_history' not in session:
                session['transformations_history'] = []
            
            session['transformations_history'].append({
                'type': transformation_data['type'],
                'details': metadata,
                'timestamp': timestamp
            })

            # Pr√©parer la r√©ponse
            return jsonify({
                'success': True,
                'metadata': {
                    'transformation_type': transformation_data['type'],
                    'details': metadata,
                    'new_file': new_filename
                },
                'redirect': url_for('data_preview')  # Rediriger vers l'aper√ßu
            })

        except Exception as transform_error:
            logger.error(f"Erreur de transformation : {transform_error}")
            return jsonify({
                'success': False, 
                'error': f'Erreur lors de la transformation : {str(transform_error)}'
            }), 500

    except Exception as e:
        logger.error(f"Erreur inattendue : {e}")
        return jsonify({
            'success': False, 
            'error': f'Erreur inattendue : {str(e)}'
        }), 500

@app.route('/get_transformations', methods=['GET'])
def get_transformations():
    """
    R√©cup√®re l'historique des transformations
    """
    return jsonify({
        'history': session.get('transformations_history', [])
    })

@app.route('/rollback_transformation', methods=['POST'])
def rollback_transformation():
    """
    Annule la derni√®re transformation
    """
    try:
        # V√©rifier s'il y a un historique de transformations
        if 'transformations_history' not in session or not session['transformations_history']:
            return jsonify({
                'success': False,
                'error': 'Aucune transformation √† annuler'
            }), 400

        # Supprimer la derni√®re transformation
        last_transformation = session['transformations_history'].pop()
        session.modified = True

        # Charger le fichier pr√©c√©dent (si possible)
        if len(session['transformations_history']) > 0:
            # Revenir au fichier avant la derni√®re transformation
            previous_file = os.path.join(
                app.config['UPLOAD_FOLDER'], 
                session['transformations_history'][-1]['details'].get('original_filename', '')
            )
        else:
            # Revenir au fichier original
            previous_file = session.get('original_file', {}).get('path')

        if not previous_file or not os.path.exists(previous_file):
            return jsonify({
                'success': False,
                'error': 'Impossible de restaurer le fichier pr√©c√©dent'
            }), 500

        # Mettre √† jour le fichier courant
        session['current_file'] = {
            'name': os.path.basename(previous_file),
            'path': previous_file,
            'type': 'csv'
        }
        session.modified = True

        return jsonify({
            'success': True,
            'message': 'Derni√®re transformation annul√©e',
            'redirect': url_for('data_preview')
        })

    except Exception as e:
        logger.error(f"Erreur lors de l'annulation : {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500



# üìå ROUTES PRINCIPALES
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/data_processing', methods=['GET', 'POST'])
def data_processing():
    if request.method == 'POST':
        if 'file' not in request.files:
            flash('Aucun fichier trouv√©', 'error')
            return redirect(request.url)
        
        file = request.files['file']
        if file.filename == '':
            flash('Aucun fichier s√©lectionn√©', 'error')
            return redirect(request.url)
        
        if file and allowed_file(file.filename):
            filename = secure_filename(file.filename)
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            file.save(file_path)

            session['current_file'] = {'name': filename, 'path': file_path, 'type': 'csv'}

            try:
                # Code pour charger le CSV avec le d√©limiteur
                delimiter = request.form.get('delimiter', 'auto')
                if delimiter == 'auto':
                    df = pd.read_csv(file_path, sep=None, engine='python')
                else:
                    df = pd.read_csv(file_path, sep=delimiter)
                
                # Convertir les types NumPy pour la s√©rialisation JSON
                df_info = {
                    'shape': list(df.shape),
                    'columns': df.columns.tolist(),
                    'dtypes': {col: str(df[col].dtype) for col in df.columns},
                    'missing_values': int(df.isna().sum().sum()),
                    'has_numeric': bool(any(pd.api.types.is_numeric_dtype(df[col]) for col in df.columns))
                }
                
                print(f"Colonnes d√©tect√©es: {df.columns.tolist()}")
                # Convertir toutes les valeurs NumPy
                session['df_info'] = convert_numpy_types(df_info)
                
                flash(f'Fichier "{filename}" charg√© avec succ√®s!', 'success')
                return redirect(url_for('data_preview'))
            except Exception as e:
                logger.error(f"Erreur lors du chargement du fichier: {e}")
                flash(f'Erreur lors du chargement du fichier: {e}', 'error')
                return redirect(request.url)  # Ajoutez cette ligne pour g√©rer l'erreur
    
    # N'oubliez pas de retourner quelque chose √† la fin de la fonction
    return render_template('data_processing.html')

@app.route('/data_preview')
def data_preview():
    if 'current_file' not in session or session['current_file']['type'] != 'csv':
        flash('Veuillez d\'abord charger un fichier CSV', 'warning')
        return redirect(url_for('data_processing'))

    file_path = session['current_file']['path']

    try:
        df = pd.read_csv(file_path)
        preview_data = df.head(100)
        
        # R√©cup√©rer les informations sur le dataframe
        df_info = session['df_info']
        
        # Identifier et compter les colonnes num√©riques
        numeric_columns = []
        categorical_columns = []
        
        for col in df.columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                numeric_columns.append(col)
            else:
                categorical_columns.append(col)
        
        # Mettre √† jour les informations
        df_info['numeric_count'] = len(numeric_columns)
        df_info['has_numeric'] = len(numeric_columns) > 0
        df_info['numeric_columns'] = numeric_columns
        df_info['categorical_columns'] = categorical_columns
        
        # Sauvegarder les modifications dans la session
        session['df_info'] = df_info

        return render_template(
            'data_preview.html',
            filename=session['current_file']['name'],
            df_info=df_info,
            preview_data=preview_data.to_html(classes='table table-striped table-hover', index=False),
            columns=df.columns.tolist(),
            numeric_columns=numeric_columns,
            categorical_columns=categorical_columns
        )
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration de l'aper√ßu: {e}")
        flash(f'Erreur lors de la g√©n√©ration de l\'aper√ßu: {e}', 'error')
        return redirect(url_for('data_processing'))

# Ajouter cette route pour obtenir les valeurs uniques d'une colonne
@app.route('/api/column_values', methods=['POST'])
def get_column_values():
    """
    API pour r√©cup√©rer les valeurs uniques d'une colonne avec leurs fr√©quences.
    """
    if 'current_file' not in session or session['current_file']['type'] != 'csv':
        return jsonify({"error": "Aucun fichier CSV charg√©"}), 400
    
    # R√©cup√©rer les param√®tres de la requ√™te
    params = request.json
    column_name = params.get('column_name')
    
    if not column_name:
        return jsonify({"error": "Nom de colonne non sp√©cifi√©"}), 400
    
    try:
        # Charger le DataFrame
        file_path = session['current_file']['path']
        df = pd.read_csv(file_path)
        
        # V√©rifier si la colonne existe
        if column_name not in df.columns:
            return jsonify({"error": f"Colonne '{column_name}' introuvable"}), 404
        
        # Calculer les valeurs uniques et leurs fr√©quences
        value_counts = df[column_name].value_counts(dropna=False).reset_index()
        value_counts.columns = ['value', 'count']
        
        # G√©rer les valeurs NULL explicitement
        null_rows = value_counts[value_counts['value'].isna()]
        if not null_rows.empty:
            # Remplacer NaN par "NULL" pour le JSON
            null_idx = null_rows.index[0]
            value_counts.at[null_idx, 'value'] = "NULL"
        
        # Convertir en liste de dictionnaires pour le JSON
        values_data = []
        for _, row in value_counts.iterrows():
            # G√©rer les diff√©rents types de donn√©es
            if pd.isna(row['value']):
                value = "NULL"
            elif row['value'] == "":
                value = ""  # Cha√Æne vide
            else:
                value = row['value']
            
            values_data.append({
                "value": value,
                "count": int(row['count'])
            })
        
        # Ajouter des statistiques suppl√©mentaires
        stats = {
            "total_rows": len(df),
            "unique_values": df[column_name].nunique(dropna=False),
            "missing_values": int(df[column_name].isna().sum()),
            "data_type": str(df[column_name].dtype)
        }
        
        return jsonify({
            "values": values_data,
            "stats": stats
        })
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des valeurs uniques: {e}")
        return jsonify({"error": str(e)}), 500

# API pour g√©n√©rer une visualisation
@app.route('/api/generate_visualization', methods=['POST'])
def generate_visualization():
    """
    API pour g√©n√©rer une visualisation √† partir des param√®tres fournis.
    """
    if 'current_file' not in session or session['current_file']['type'] != 'csv':
        return jsonify({"error": "Aucun fichier CSV charg√©"}), 400
    
    # R√©cup√©rer les param√®tres de la requ√™te
    params = request.json
    chart_type = params.get('chart_type')
    x_var = params.get('x_var')
    y_var = params.get('y_var')
    color_var = params.get('color_var')
    
    # Param√®tres optionnels
    additional_params = {}
    for key, value in params.items():
        if key not in ['chart_type', 'x_var', 'y_var', 'color_var']:
            additional_params[key] = value
    
    try:
        # Charger le DataFrame
        file_path = session['current_file']['path']
        df = pd.read_csv(file_path)
        
        # G√©n√©rer la visualisation
        visualization_data = create_visualization(
            df, 
            chart_type, 
            x_var=x_var, 
            y_var=y_var, 
            color_var=color_var, 
            **additional_params
        )
        
        return jsonify(visualization_data)
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration de la visualisation: {e}")
        return jsonify({"error": str(e)}), 500
    

# V√©rifiez dans app.py que vous avez bien la route suivante :
@app.route('/visualizations')
def visualizations():
    if 'current_file' not in session:
        flash('Veuillez d\'abord charger un fichier', 'warning')
        return redirect(url_for('index'))

    file_path = session['current_file']['path']
    file_type = session['current_file']['type']

    if file_type != 'csv':
        flash('Les visualisations ne sont disponibles que pour les fichiers CSV', 'warning')
        return redirect(url_for('index'))

    try:
        df = pd.read_csv(file_path)
        return render_template(
            'visualizations.html',
            filename=session['current_file']['name'],
            columns=df.columns.tolist(),
            numeric_columns=df.select_dtypes(include=['number']).columns.tolist(),
            categorical_columns=df.select_dtypes(exclude=['number']).columns.tolist()
        )
    except Exception as e:
        logger.error(f"Erreur lors du chargement pour visualisation: {e}")
        flash(f'Erreur lors du chargement pour visualisation: {e}', 'error')
        return redirect(url_for('index'))

# üìå ROUTE : Transformation des donn√©es CSV
@app.route('/data_transform', methods=['GET', 'POST'])
def data_transform():
    """Page de transformation des donn√©es"""
    if 'current_file' not in session or session['current_file']['type'] != 'csv':
        flash('Veuillez d\'abord charger un fichier CSV', 'warning')
        return redirect(url_for('data_processing'))

    analysis_result = None  # Variable pour stocker le r√©sultat de l'analyse IA
    transformations_data = {}  # Variable pour stocker les d√©tails des transformations

    logger.info("D√©but de la route data_transform")
    logger.info(f"M√©thode de requ√™te : {request.method}")

    if request.method == 'POST':
        logger.info("Contenu complet du formulaire :")
        transformations = request.form.getlist('transformations')
        user_context = request.form.get('user_context', '')
        is_ai_analysis = request.form.get('is_ai_analysis') == 'true'

        # Extraire les d√©tails des transformations
        for transform in transformations:
            if transform != 'ai_analysis':
                transform_data = {}
                
                # Filtrer les cl√©s sp√©cifiques √† cette transformation
                transform_keys = [key for key in request.form.keys() if key.startswith(f"{transform}_")]
                
                for key in transform_keys:
                    # Supprimer le pr√©fixe de transformation
                    param_name = key[len(transform)+1:]
                    values = request.form.getlist(key)
                    
                    # Si c'est un tableau, le convertir en liste
                    if param_name.endswith('[]'):
                        param_name = param_name[:-2]
                        transform_data[param_name] = values
                    # Sinon, prendre la premi√®re valeur
                    else:
                        transform_data[param_name] = values[0] if values else None
                
                # Traitement sp√©cial pour les remplacements de valeurs
                if transform == 'replace_values':
                    # R√©cup√©rer les valeurs originales et nouvelles
                    original_values = request.form.getlist('original_values[]')
                    new_values = request.form.getlist('new_values[]')
                    
                    # Cr√©er un dictionnaire de remplacements
                    replacements = {}
                    for i in range(min(len(original_values), len(new_values))):
                        if original_values[i]:  # Ignorer les entr√©es vides
                            replacements[original_values[i]] = new_values[i]
                    
                    transform_data['replacements'] = replacements
                    transform_data['replace_all'] = request.form.get('replace_all_occurrences') == 'on'
                
                # Ne stocker que les transformations non vides
                if transform_data:
                    transformations_data[transform] = transform_data

        #log pour d√©boguer
        logger.info(f"Transformations d√©tect√©es : {transformations_data}")

        file_path = session['current_file']['path']

        try:
            df = pd.read_csv(file_path)
            processor = DataProcessor()

            # Si c'est une analyse IA, effectuer l'analyse en conservant les transformations
            if is_ai_analysis and user_context:
                # Appeler la m√©thode d'analyse IA
                analysis_result, metadata = processor.analyze_with_ai(df, transformations, user_context)
                logger.info(f"r√©sultats d'analyse {analysis_result}")
                
                # Sauvegarder l'analyse dans l'historique si demand√©
                if request.form.get('save_history') == 'true':
                    dataset_name = session['current_file']['name']
                    csv_history.add_analysis(
                        dataset_name=dataset_name,
                        dataset_description=f"Analyse IA: {user_context[:50]}{'...' if len(user_context) > 50 else ''}",
                        analysis_text=analysis_result,
                        metadata={'transformations': transformations, 'user_context': user_context}
                    )
                    flash('Analyse IA sauvegard√©e dans l\'historique!', 'success')
                
                # Ne pas rediriger - continuer pour afficher la page avec les r√©sultats
            
            # Si ce n'est pas une analyse IA ou si on veut appliquer les transformations
            elif not is_ai_analysis:
                transformed_df, metadata = processor.process_dataframe(df, transformations_data, user_context)
                
                # Sauvegarder le DataFrame transform√©
                transformed_filename = f"transformed_{session['current_file']['name']}"
                transformed_path = os.path.join(app.config['UPLOAD_FOLDER'], transformed_filename)
                transformed_df.to_csv(transformed_path, index=False)
                
                # Mettre √† jour les informations du fichier en session
                session['current_file'] = {
                    'name': transformed_filename,
                    'path': transformed_path,
                    'type': 'csv'
                }
                
                # Effacer les transformations car elles ont √©t√© appliqu√©es
                transformations_data = {}
                
                # Sauvegarder dans l'historique si demand√©
                if request.form.get('save_history') == 'true':
                    dataset_name = session['current_file']['name']
                    analysis_text = metadata.get('analysis', 'Analyse non disponible')
                    csv_history.add_analysis(
                        dataset_name=dataset_name,
                        dataset_description=f"Transformation avec {len(transformations)} op√©rations",
                        analysis_text=analysis_text,
                        metadata={'transformations': transformations}
                    )
                
                flash('Transformations appliqu√©es avec succ√®s!', 'success')
                return redirect(url_for('data_preview'))
                
        except Exception as e:
            logger.error(f"Erreur lors de la transformation/analyse: {e}")
            flash(f'Erreur lors de la transformation/analyse: {e}', 'error')

    # R√©cup√©rer les donn√©es pour le formulaire - m√©thode GET ou apr√®s analyse IA
    try:
        # Charger le fichier CSV
        file_path = session['current_file']['path']
        df = pd.read_csv(file_path)
        
        # R√©cup√©rer df_info de la session ou le recr√©er
        if 'df_info' in session:
            df_info = session['df_info']
        else:
            # Cr√©er les informations de base sur le dataframe
            df_info = {
                'shape': list(df.shape),
                'columns': df.columns.tolist(),
                'dtypes': {col: str(df[col].dtype) for col in df.columns},
                'missing_values': int(df.isna().sum().sum())
            }
        
        # Identifier les colonnes num√©riques et cat√©gorielles
        numeric_columns = []
        categorical_columns = []
        
        for col in df.columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                numeric_columns.append(col)
            else:
                categorical_columns.append(col)
        
        # Mettre √† jour les informations
        df_info['numeric_count'] = len(numeric_columns)
        df_info['has_numeric'] = len(numeric_columns) > 0
        
        # Passer les variables au template, y compris le r√©sultat d'analyse IA s'il existe
        return render_template(
            'data_transform.html', 
            df_info=df_info,
            numeric_columns=numeric_columns,
            categorical_columns=categorical_columns,
            analysis_result=analysis_result,  # Passer le r√©sultat d'analyse
            transformations_data=transformations_data  # Passer les d√©tails des transformations
        )
    
    except Exception as e:
        logger.error(f"Erreur lors du chargement pour transformation: {e}")
        flash(f'Erreur lors du chargement pour transformation: {e}', 'error')
        return redirect(url_for('data_preview'))
    


# üìå ROUTE : Page de traitement des PDF
@app.route('/pdf_processing', methods=['GET', 'POST'])
def pdf_processing():
    return render_template('pdf_processing.html')

@app.route('/process_pdf', methods=['POST'])
def process_pdf():
    """Traitement du fichier PDF via le formulaire principal"""
    if 'pdf_file' not in request.files:
        flash('Aucun fichier trouv√©', 'error')
        return redirect(url_for('pdf_processing'))
    
    file = request.files['pdf_file']
    if file.filename == '':
        flash('Aucun fichier s√©lectionn√©', 'error')
        return redirect(url_for('pdf_processing'))
    
    filename = secure_filename(file.filename)
    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
    file.save(file_path)
    
    # Stocker les informations du fichier en session
    session['current_file'] = {'name': filename, 'path': file_path, 'type': 'pdf'}
    
    # R√©cup√©rer le type d'analyse demand√©
    analysis_type = request.form.get('analysis_type', 'summary')
    
    try:
        processor = PDFProcessor()
        pdf_result = processor.process_pdf(file_path, context=analysis_type)
        
        if not pdf_result.get("success", False):
            flash(f"Erreur lors de l'analyse: {pdf_result.get('error', 'Erreur inconnue')}", 'error')
            return redirect(url_for('pdf_processing'))
        
        # Stocker les r√©sultats en session
        session['pdf_analysis'] = pdf_result
        
        # Enregistrer dans l'historique si r√©ussi
        if pdf_result.get("success", False):
            pdf_history.add_pdf_analysis(
                pdf_id=pdf_result.get("pdf_id", str(uuid.uuid4())),
                pdf_name=filename,
                analysis_result=pdf_result.get("analysis", {}),
                metadata=pdf_result.get("metadata", {})
            )
        
        return redirect(url_for('pdf_results'))
    
    except Exception as e:
        logger.error(f"Erreur lors du traitement du PDF: {e}")
        flash(f"Erreur lors du traitement: {e}", 'error')
        return redirect(url_for('pdf_processing'))

# üìå ROUTE : Analyse des PDFs
@app.route('/pdf_analysis', methods=['GET', 'POST'])
def pdf_analysis():
    # Si c'est une m√©thode GET, rediriger vers le formulaire
    if request.method == 'GET':
        return redirect(url_for('pdf_processing'))
    
    # Le reste du code reste identique
    logger.info("üì• Requ√™te re√ßue pour analyse PDF")

    if 'file' not in request.files:
        logger.error("‚ùå Aucun fichier re√ßu")
        return jsonify({"error": "Aucun fichier re√ßu"}), 400
    
    # ... le reste de votre code ...

    file = request.files['file']

    if file.filename == '':
        logger.error("‚ùå Aucun fichier s√©lectionn√©")
        return jsonify({"error": "Aucun fichier s√©lectionn√©"}), 400

    logger.info(f"‚úÖ Fichier re√ßu : {file.filename}")


    # Sauvegarde du fichier
    filename = secure_filename(file.filename)
    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
    file.save(file_path)
    logger.info(f"‚úÖ Fichier enregistr√© sous : {file_path}")

    # V√©rifier si le fichier est bien un PDF
    is_valid_pdf = False
    with open(file_path, 'rb') as f:
        if f.read(4).startswith(b'%PDF'):
            is_valid_pdf = True

    if not is_valid_pdf:
        logger.error("‚ùå Le fichier ne semble pas √™tre un PDF valide")
        return jsonify({"error": "Le fichier ne semble pas √™tre un PDF valide"}), 400

    # Charger le contexte utilisateur
    user_context = request.form.get('user_context', '')

    try:
        processor = PDFProcessor()
        logger.info("üõ† D√©but de l'analyse du PDF...")

        pdf_result = processor.process_pdf(file_path, context=user_context)

        # V√©rifier si l'analyse a r√©ussi
        if not pdf_result.get("success", False):
            logger.error(f"‚ùå √âchec de l'analyse : {pdf_result.get('error', 'Erreur inconnue')}")
            return jsonify({"error": pdf_result.get("error", "Erreur inconnue")}), 500

        # Ajouter cette partie ici:
        session['current_file'] = {'name': filename, 'path': file_path, 'type': 'pdf'}
        session['pdf_analysis'] = pdf_result
        
        # Enregistrer dans l'historique
        if pdf_result.get("success", False):
            pdf_id = pdf_result.get("pdf_id", str(uuid.uuid4()))
            pdf_history.add_pdf_analysis(
                pdf_id=pdf_id,
                pdf_name=filename,
                analysis_result=pdf_result.get("analysis", {}),
                metadata=pdf_result.get("metadata", {})
            )
        
        logger.info("‚úÖ Analyse termin√©e avec succ√®s !")
        return jsonify(pdf_result)

    except Exception as e:
        logger.exception("‚ùå Erreur critique lors de l'analyse du PDF")
        return jsonify({"error": f"Exception: {str(e)}"}), 500

# üìå ROUTE : Affichage des r√©sultats PDF
@app.route('/pdf_results')
def pdf_results():
    if 'pdf_analysis' not in session:
        flash('Aucune analyse PDF en cours', category='warning')
        return redirect(url_for('pdf_analysis'))

    analysis = session['pdf_analysis']
    filename = session['current_file']['name']

    return render_template(
        'pdf_results.html',
        filename=filename,
        metadata=analysis['metadata'],
        analysis=analysis['analysis'],
        tables=analysis['tables']
    )

# üìå ROUTES HISTORIQUE ET PARAM√àTRES
@app.route('/history')
def history():
    csv_analyses = csv_history.get_recent_analyses(limit=10)
    pdf_analyses = pdf_history.get_recent_pdf_analyses(limit=10)
    
    # S√©parer les analyses normales des analyses IA pour mieux les pr√©senter
    ai_analyses = []
    regular_analyses = []
    
    for analysis in csv_analyses:
        if "Analyse IA" in analysis.get("dataset_description", ""):
            ai_analyses.append(analysis)
        else:
            regular_analyses.append(analysis)
    
    return render_template(
        'history.html',
        csv_analyses=regular_analyses,
        ai_analyses=ai_analyses,
        pdf_analyses=pdf_analyses
    )

@app.route('/settings', methods=['GET', 'POST'])
def settings():
    if request.method == 'POST':
        session['settings'] = {
            'model_name': request.form.get('model_name', 'mistral:latest'),
            'context_size': int(request.form.get('context_size', '4096')),
            'use_history': request.form.get('use_history') == 'true',
            'max_history': int(request.form.get('max_history', '3'))
        }
        
        flash('Param√®tres mis √† jour avec succ√®s!', 'success')
        return redirect(url_for('settings'))

    current_settings = session.get('settings', {
        'model_name': os.environ.get('AI_MODEL', 'mistral:latest'),
        'context_size': int(os.environ.get('CONTEXT_SIZE', '4096')),
        'use_history': True,
        'max_history': 3
    })
    
    return render_template('settings.html', settings=current_settings)

@app.route('/clear_history', methods=['POST'])
def clear_history():
    # Logique pour effacer l'historique ici
    csv_history.clear_history()  # Supposons que vous ayez une m√©thode pour cela
    pdf_history.clear_history()  # De m√™me pour l'historique PDF
    flash('L\'historique a √©t√© effac√© avec succ√®s.', 'success')
    return redirect(url_for('settings'))

@app.route('/cartography')
def cartography():
    """Page de cartographie pour visualiser les donn√©es g√©or√©f√©renc√©es."""
    if 'current_file' not in session:
        flash('Veuillez d\'abord charger un fichier', 'warning')
        return redirect(url_for('index'))

    file_path = session['current_file']['path']
    file_type = session['current_file']['type']

    if file_type != 'csv':
        flash('La cartographie n\'est disponible que pour les fichiers CSV', 'warning')
        return redirect(url_for('index'))

    try:
        # R√©cup√©rer les informations du fichier depuis la session
        df_info = session.get('df_info', {})
        
        # Utiliser le d√©limiteur stock√© dans la session ou utiliser un point-virgule par d√©faut
        delimiter = df_info.get('delimiter_used', ';')
        
        logger.info(f"Tentative de chargement du fichier avec d√©limiteur '{delimiter}'")
        
        # Charger le DataFrame avec le bon d√©limiteur
        try:
            # Tenter d'abord avec le d√©limiteur stock√©
            df = pd.read_csv(file_path, sep=delimiter)
            
            # V√©rifier que plusieurs colonnes sont d√©tect√©es
            if len(df.columns) <= 1 and delimiter != ';':
                # Si une seule colonne est d√©tect√©e, essayer avec le point-virgule
                logger.info("Une seule colonne d√©tect√©e, tentative avec d√©limiteur ';'")
                df = pd.read_csv(file_path, sep=';')
                delimiter = ';'
        except Exception as e:
            # En cas d'erreur, essayer avec le point-virgule
            logger.warning(f"Erreur avec le d√©limiteur initial: {e}")
            df = pd.read_csv(file_path, sep=';')
            delimiter = ';'
        
        # V√©rifier √† nouveau le nombre de colonnes
        if len(df.columns) <= 1:
            # Si toujours une seule colonne, essayer avec l'auto-d√©tection
            logger.warning("Toujours une seule colonne d√©tect√©e, tentative avec auto-d√©tection")
            df = pd.read_csv(file_path, sep=None, engine='python')
            delimiter = 'auto'
        
        # D√©terminer les colonnes potentiellement g√©ographiques
        geo_columns = []
        for col in df.columns:
            col_lower = str(col).lower()
            if any(term in col_lower for term in ['lat', 'latitude', 'lng', 'longitude', 'lon', 'coord', 'gps', 'x', 'y']):
                geo_columns.append(col)
        
        # D√©terminer les colonnes num√©riques (pour des visualisations potentielles)
        numeric_columns = df.select_dtypes(include=['number']).columns.tolist()
        
        # Si aucune colonne num√©rique n'est d√©tect√©e, essayer de convertir
        if not numeric_columns:
            logger.info("Aucune colonne num√©rique d√©tect√©e, tentative de conversion")
            for col in df.columns:
                try:
                    # Ignorer les colonnes d√©j√† identifi√©es comme g√©ographiques
                    if col not in geo_columns:
                        # Essayer de convertir en num√©rique
                        numeric_series = pd.to_numeric(df[col], errors='coerce')
                        # Si au moins 50% des valeurs sont num√©riques, consid√©rer comme colonne num√©rique
                        if numeric_series.notna().mean() >= 0.5:
                            numeric_columns.append(col)
                except:
                    pass
        
        # Journaliser les informations pour le d√©bogage
        logger.info(f"D√©limiteur utilis√©: '{delimiter}'")
        logger.info(f"Nombre total de colonnes: {len(df.columns)}")
        logger.info(f"Colonnes: {df.columns.tolist()}")
        logger.info(f"Colonnes g√©ographiques probables: {geo_columns}")
        logger.info(f"Colonnes num√©riques: {numeric_columns}")
        
        # Retourner le template avec les colonnes disponibles
        return render_template(
            'cartography.html',
            filename=session['current_file']['name'],
            columns=df.columns.tolist(),
            geo_columns=geo_columns,
            numeric_columns=numeric_columns,
            num_rows=len(df),
            delimiter_used=delimiter
        )
    except Exception as e:
        logger.error(f"Erreur lors du chargement pour cartographie: {e}", exc_info=True)
        flash(f'Erreur lors du chargement pour cartographie: {e}', 'error')
        return redirect(url_for('index'))

@app.route('/api/generate_map', methods=['POST'])
def generate_map():
    """API pour g√©n√©rer des donn√©es cartographiques."""
    if 'current_file' not in session or session['current_file']['type'] != 'csv':
        return jsonify({"error": "Aucun fichier CSV charg√©"}), 400
    
    # R√©cup√©rer les param√®tres de la requ√™te
    params = request.json
    lat_col = params.get('lat_col')
    lng_col = params.get('lng_col')
    label_col = params.get('label_col')
    
    if not lat_col or not lng_col:
        return jsonify({"error": "Veuillez s√©lectionner les colonnes de latitude et longitude"}), 400
    
    try:
        # Charger le DataFrame
        file_path = session['current_file']['path']
        
        # Essayer avec une virgule comme d√©limiteur (pour les fichiers CSV standards)
        try:
            df = pd.read_csv(file_path, sep=',')
            logger.info(f"Fichier charg√© avec d√©limiteur ',' - {len(df.columns)} colonnes d√©tect√©es")
        except Exception as e:
            # Si √ßa √©choue, essayer avec le point-virgule
            logger.warning(f"√âchec de chargement avec ',': {e}")
            df = pd.read_csv(file_path, sep=';')
            logger.info(f"Fichier charg√© avec d√©limiteur ';' - {len(df.columns)} colonnes d√©tect√©es")
        
        # V√©rifier les colonnes disponibles (pour le d√©bogage)
        logger.info(f"Colonnes disponibles: {df.columns.tolist()}")
        
        # Cr√©er un dictionnaire pour faire correspondre les noms de colonnes insensibles √† la casse
        col_map = {col.lower(): col for col in df.columns}
        
        # Trouver les colonnes r√©elles correspondant aux noms demand√©s
        real_lat_col = col_map.get(lat_col.lower())
        real_lng_col = col_map.get(lng_col.lower())
        
        # Si les colonnes exactes ne sont pas trouv√©es, chercher des alternatives
        if not real_lat_col:
            # Essayer de trouver une colonne qui contient "lat"
            for col in df.columns:
                if "lat" in col.lower():
                    real_lat_col = col
                    break
        
        if not real_lng_col:
            # Essayer de trouver une colonne qui contient "lon"
            for col in df.columns:
                if any(x in col.lower() for x in ["lon", "lng", "long"]):
                    real_lng_col = col
                    break
        
        # V√©rifier si on a trouv√© les colonnes
        if not real_lat_col:
            return jsonify({"error": f"Colonne de latitude '{lat_col}' introuvable. Colonnes disponibles: {', '.join(df.columns)}"}), 400
        if not real_lng_col:
            return jsonify({"error": f"Colonne de longitude '{lng_col}' introuvable. Colonnes disponibles: {', '.join(df.columns)}"}), 400
        
        # Utiliser les noms r√©els des colonnes
        lat_col = real_lat_col
        lng_col = real_lng_col
        
        # Traiter la colonne d'√©tiquette
        real_label_col = None
        if label_col and label_col != "Aucune":
            real_label_col = col_map.get(label_col.lower())
            if not real_label_col:
                # Si l'√©tiquette n'est pas trouv√©e, utiliser la premi√®re colonne de texte disponible
                for col in df.columns:
                    if col != lat_col and col != lng_col and df[col].dtype == 'object':
                        real_label_col = col
                        break
            label_col = real_label_col
        
        # Convertir les colonnes de coordonn√©es en num√©rique
        df[lat_col] = pd.to_numeric(df[lat_col], errors='coerce')
        df[lng_col] = pd.to_numeric(df[lng_col], errors='coerce')
        
        # Filtrer les valeurs non valides
        valid_mask = ~df[lat_col].isna() & ~df[lng_col].isna()
        
        # Ajouter des v√©rifications de plage pour les coordonn√©es
        valid_mask = valid_mask & (df[lat_col] >= -90) & (df[lat_col] <= 90)
        valid_mask = valid_mask & (df[lng_col] >= -180) & (df[lng_col] <= 180)
        
        df_valid = df[valid_mask].copy()
        
        if len(df_valid) == 0:
            return jsonify({
                "error": "Aucune coordonn√©e valide trouv√©e. V√©rifiez que vos colonnes contiennent bien des coordonn√©es g√©ographiques."
            }), 400
        
        # Informations pour le d√©bogage
        logger.info(f"Points valides: {len(df_valid)} sur {len(df)} lignes")
        logger.info(f"Colonnes utilis√©es - Latitude: {lat_col}, Longitude: {lng_col}, √âtiquette: {label_col}")
        
        # Pr√©parer les donn√©es pour la carte (format GeoJSON)
        features = []
        for idx, row in df_valid.iterrows():
            try:
                lat = float(row[lat_col])
                lng = float(row[lng_col])
                
                # Propri√©t√©s de base
                properties = {
                    "id": int(idx),
                    "lat": lat,
                    "lng": lng
                }
                
                # Ajouter l'√©tiquette si sp√©cifi√©e
                if label_col and label_col != "Aucune":
                    properties["label"] = str(row[label_col]) if pd.notna(row[label_col]) else f"Point {idx}"
                else:
                    properties["label"] = f"Point {idx}"
                
                # Ajouter d'autres informations utiles
                for col in df.columns:
                    if col != lat_col and col != lng_col and col != label_col:
                        properties[col] = str(row[col]) if pd.notna(row[col]) else ""
                
                # Ajouter le point au GeoJSON
                feature = {
                    "type": "Feature",
                    "geometry": {
                        "type": "Point",
                        "coordinates": [lng, lat]
                    },
                    "properties": properties
                }
                
                features.append(feature)
            except Exception as e:
                logger.warning(f"Erreur lors du traitement du point {idx}: {e}")
                continue
        
        if not features:
            return jsonify({
                "error": "Impossible de g√©n√©rer des points valides avec les colonnes s√©lectionn√©es."
            }), 400
        
        # Calculer le centre de la carte
        lats = [f["geometry"]["coordinates"][1] for f in features]
        lngs = [f["geometry"]["coordinates"][0] for f in features]
        
        center = [
            sum(lngs) / len(lngs),
            sum(lats) / len(lats)
        ]
        
        # Renvoyer les donn√©es
        return jsonify({
            "map_data": {
                "type": "FeatureCollection",
                "features": features
            },
            "center": center,
            "stats": {
                "total_points": len(df),
                "valid_points": len(features),
                "invalid_points": len(df) - len(features)
            }
        })
        
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration de la carte: {e}", exc_info=True)
        return jsonify({"error": f"Erreur: {str(e)}"}), 500

# üìå ROUTE : Transcription audio

@app.route('/audio_transcription', methods=['GET', 'POST'])
def audio_transcription():
    if request.method == 'POST':
        # V√©rifier si un fichier audio est upload√©
        if 'audio_file' not in request.files:
            flash('Aucun fichier audio trouv√©', 'error')
            return redirect(request.url)
        
        audio_file = request.files['audio_file']
        target_language = request.form.get('target_language', None)
        
        # Sauvegarder temporairement le fichier
        filename = secure_filename(audio_file.filename)
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        audio_file.save(file_path)
        
        try:
            # Chemins √† configurer selon votre installation
            WHISPER_MODEL = 'base'  # ou chemin local
            TRANSLATION_MODEL = 'Mistral:latest'
            
            # Initialiser le processeur audio
            processor = LocalAudioProcessor(
                whisper_model_size='base',
                translation_model_path= 'Mistral:latest'
            )
            
            # Traiter l'audio
            result = processor.process_audio(
                file_path, 
                target_language=target_language
            )
            
            # Nettoyer le fichier temporaire
            os.remove(file_path)
            
            return render_template(
                'audio_transcription_results.html', 
                result=result
            )
        
        except Exception as e:
            logger.error(f"Erreur de transcription : {e}")
            flash(f'Erreur lors du traitement audio : {e}', 'error')
            return redirect(request.url)
    
    # Page de formulaire pour l'upload
    return render_template('audio_transcription.html')


@app.template_filter('wordcount')
def wordcount(text):
    """Compte le nombre de mots dans un texte"""
    return len(str(text).split())


@app.route('/signal_analysis')
def signal_analysis():
    return render_template('signal_analysis.html')

#analyse du signal audio

@app.route('/analyze_audio_signal', methods=['POST'])
def analyze_audio_signal():
    logger.info("D√©but de l'analyse de signal audio")
    
    if 'audio_file' not in request.files:
        logger.error("Aucun fichier audio trouv√©")
        return jsonify({"error": "Aucun fichier audio trouv√©"}), 400
    
    audio_file = request.files['audio_file']
    logger.info(f"Fichier re√ßu : {audio_file.filename}")
    
    file_path = os.path.join(app.config['UPLOAD_FOLDER'], secure_filename(audio_file.filename))
    audio_file.save(file_path)
    
    try:
        # Analyse du signal
        logger.info(f"D√©but du traitement du fichier : {file_path}")
        analysis_result = process_audio_signal(file_path)
        return jsonify(analysis_result)
    
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse du signal : {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500
    finally:
        # Nettoyer le fichier temporaire
        if os.path.exists(file_path):
            os.remove(file_path)

# üìå LANCEMENT DE L'APPLICATION
if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(debug=True, host='0.0.0.0', port=port)